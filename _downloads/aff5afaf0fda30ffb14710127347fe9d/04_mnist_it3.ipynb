{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# IT3 for MNIST Classification\n\nIn this tutorial, we explore how the idempotence-based [Idempotent Test-Time Training (IT3)](https://www.norange.io/projects/ittt/) approach can improve model performance during inference when data is corrupted by Gaussian noise, using the [torch-ttt](https://github.com/nikitadurasov/torch-ttt) *Engine* functionality \u2014 specifically the [IT3Engine](https://github.com/nikitadurasov/torch-ttt) class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Uncomment the line below to install torch-ttt if you're running this in Colab\")\n# !pip install git+https://github.com/nikitadurasov/torch-ttt.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we will start by training a networks on MNIST data and demonstrate how susceptible it is to noise introduced during testing, resulting in significantly lower accuracy when the noise is present.\n\nLet\u2019s start with some global parameters that we will use later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision\nimport numpy as np\nimport random\nimport tqdm\nimport matplotlib.pyplot as plt\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nn_epochs = 1  # number of training epochs\nbatch_size_train = 256  # batch size during training\nbatch_size_test = 256  # batch size during training\nlearning_rate = 0.01  # training learning rate\n\nrandom_seed = 7\ntorch.backends.cudnn.enabled = False\ntorch.manual_seed(random_seed)\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\n\n# sphinx_gallery_thumbnail_path = '_static/images/examples/it3_mnist.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\nWe will employ a fairly shallow and simple model, consisting only of several convolutional and linear layers with LeakyReLU activations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 50)\n        self.fc3 = nn.Linear(50, 50)\n        self.fc4 = nn.Linear(50, 10)\n        self.activation = nn.LeakyReLU()\n\n    def forward(self, x):\n        x = self.activation(F.max_pool2d(self.conv1(x), 2))\n        x = self.activation(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 320)\n        x = self.activation(self.fc1(x))\n        x = self.activation(self.fc2(x)) + x\n        x = self.activation(self.fc3(x)) + x\n        x = self.fc4(x)\n        return F.softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data\nFor training and testing data, we will use the standard train/test MNIST split, which consists of roughly 60,000 samples for training and 10,000 for testing, and we will normalize the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n    torchvision.datasets.MNIST(\n        \"./MNIST/\",\n        train=True,\n        download=True,\n        transform=torchvision.transforms.Compose(\n            [\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n            ]\n        ),\n    ),\n    batch_size=batch_size_train,\n    shuffle=False,\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    torchvision.datasets.MNIST(\n        \"./MNIST/\",\n        train=False,\n        download=True,\n        transform=torchvision.transforms.Compose(\n            [\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n            ]\n        ),\n    ),\n    batch_size=batch_size_test,\n    shuffle=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let\u2019s visualize 10 random images from the clean MNIST test set. As we can see, the digits are clearly visible and distinctive.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "images, _ = next(iter(test_loader))\nfig, axes = plt.subplots(1, 10, figsize=(15, 2))\nfor ax, img in zip(axes, images[:10, 0]):\n    ax.imshow(img, cmap=\"viridis\")\n    ax.axis(\"off\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let\u2019s add a significant amount of Gaussian noise to our input images, which will make the classification task significantly more challenging. Let\u2019s plot how they look.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "images, _ = next(iter(test_loader))\nfig, axes = plt.subplots(1, 10, figsize=(15, 2))\nfor ax, img in zip(axes, images[:10, 0]):\n    noisy_img = img + 2 * torch.randn(img.shape)\n    ax.imshow(noisy_img, cmap=\"viridis\")\n    ax.axis(\"off\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Idempotent Test-Time Training\nNow, let\u2019s apply the IT3 approach to enhance performance on noisy inputs. We\u2019ll use the [IT3Engine](https://github.com/nikitadurasov/torch-ttt) class, which handles idempotence loss computation and gradient-based optimization during inference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_ttt.engine.it3_engine import IT3Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To integrate IT3 into our existing training and testing pipelines, we only need to add a few lines of code. In the cells below, the creation of the IT3 engine is marked with comments for clarity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "network = Net() # Create the original model\n\n# Create the engine wrapper that encapsulates all IT3 logic\nengine = IT3Engine(\n\n    # Original model\n    model=network,\n\n    # The layer to which output features will be added\n    features_layer_name=\"conv1\",\n\n    # Distance function used to estimate idempotence between two outputs\n    distance_fn=lambda x, y: -torch.sum(y * x, dim=1).mean()\n)\n\n# Optimize the engine (which includes TTT logic), not the base model directly\noptimizer = optim.Adam(engine.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\nThe functions below implement standard training and evaluation loops, adapted to use the IT3 engine. During training, the self-supervised idempotence loss is combined with the classification loss. During testing, the model is evaluated on noisy inputs using IT3's inference-time optimization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train():\n    engine.train()  # Switch engine to training mode\n    correct = 0\n    counter = 0\n    with tqdm.tqdm(total=len(train_loader), desc=\"Train\") as pbar:\n        for batch_idx, (data, target) in enumerate(train_loader):\n\n            optimizer.zero_grad()\n\n            # Following the original IT3, pass ground truth as\n            # additional input during inference\n            one_hot = F.one_hot(target, num_classes=10).float()\n\n            # The engine returns the model output and the\n            # self-supervised loss (idempotence in the case of IT3)\n            output, loss_ttt = engine(data, target=one_hot)\n\n            loss = (\n                F.nll_loss((output + 1e-8).log(), target) + loss_ttt\n            )  # Add the self-supervised loss to the total loss\n\n            pred = output.data.max(1, keepdim=True)[1]\n            correct += pred.eq(target.data.view_as(pred)).sum().item()\n            counter += len(data)\n\n            loss.backward()\n            optimizer.step()\n\n            if (batch_idx + 1) % 10 == 0:\n                pbar.update(10)\n                pbar.set_postfix(acc=correct / counter)\n\ndef ttt_test():\n    batch_accs = []\n    engine.eval()  # Switch engine to evaluation mode\n    correct = 0\n    with tqdm.tqdm(total=len(test_loader), desc=\"Test\") as pbar:\n        for batch_idx, (data, target) in enumerate(test_loader):\n            data += 2 * torch.randn(data.shape)  # Add Gaussian noise to the input\n            output, _ = engine(data)  # Run inference with the engine\n            pred = output.data.max(1, keepdim=True)[1]\n            batch_acc = pred.eq(target.data.view_as(pred)).sum().item()\n            batch_accs += [batch_acc / len(target)]\n            correct += batch_acc\n            if (batch_idx + 1) % 100 == 0:\n                pbar.update(100)\n\n        accuracy = 100.0 * correct / len(test_loader.dataset)\n        pbar.set_postfix(acc=accuracy)\n        return batch_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for _ in range(n_epochs+1): # Run training for 2 epochs\n    train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\nWe first evaluate the model without any test-time optimization by setting the number of adaptation steps to zero.  \n\nAs we can see, due to the added noise, the model\u2019s accuracy drops significantly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"### No optimization ###\")\nengine.optimization_parameters[\"num_steps\"] = 0\nbatch_accs_no = ttt_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we enable IT3 by setting the number of optimization steps to 1 and specifying a learning rate. This significantly improves performance compared to the non-optimized model on noisy data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"### Number of optimization steps: 1 ###\")\nengine.optimization_parameters[\"num_steps\"] = 1\nengine.optimization_parameters[\"lr\"] = 1e-3\nbatch_accs_o = ttt_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The boxplot below compares the model\u2019s performance on noisy data with and without IT3 adaptation. IT3 optimization leads to a clear improvement in accuracy, showing its effectiveness in mitigatingcorruption-induced degradation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\n\nplt.boxplot(\n    [batch_accs_no, batch_accs_o],\n    widths=0.5,\n    meanline=True,\n    showmeans=True,\n    boxprops=dict(linewidth=1.5),\n    whiskerprops=dict(linewidth=1.2),\n    capprops=dict(linewidth=1.2),\n    medianprops=dict(linewidth=1.5, color='orange'),\n    meanprops=dict(linewidth=1.5, color='red')\n)\n\nplt.xticks([1, 2], ['Not Optimized', 'IT3 Optimized'])\nplt.title(\"Accuracy on Corrupted MNIST\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}