{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# ImageNet Robustness with TENT\n\nDiscover how test-time training can improve model performance on corrupted versions of ImageNet using `torch-ttt`.\n\nIn this tutorial, we demonstrate how test-time adaptation methods such as [Tent](https://arxiv.org/abs/2002.04765) can help restore accuracy when evaluating models on [ImageNet-C](https://github.com/hendrycks/robustness), a corrupted variant of the ImageNet dataset. We leverage the [torch-ttt](https://github.com/nikitadurasov/torch-ttt) library for easy and flexible integration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Uncomment the line below to install torch-ttt if you're running this in Colab\")\n# !pip install git+https://github.com/nikitadurasov/torch-ttt.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\nWe will work with the ImageNet dataset, using both the clean validation set and the corrupted validation set from [ImageNet-C](https://github.com/hendrycks/robustness). The corrupted version includes various types of distortions, such as noise, blur, and weather effects. For this tutorial, we'll focus on a single corruption type: pixelation.\n\nLet's start by downloading both the clean and corrupted versions of the ImageNet validation set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\n# Download and extract clean ImageNet validation set\nos.system(\"gdown 1Q6wQbNZMF0XdopzQGePDQAqVusMpbKxI && tar -xzf imagenet_clean.tar.gz\")\n\n# Download and extract corrupted ImageNet validation set\nos.system(\"gdown 1AeMUO_6M0F0E7AOBxpb97svMXAwjtyXf && tar -xzf imagenet_corrupted.tar.gz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... and importing the necessary libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nimport json\nimport urllib.request\nimport matplotlib.pyplot as plt\nfrom torchvision import models\n\nos.environ['TORCH_HOME'] = './weights'\nos.makedirs(os.environ['TORCH_HOME'], exist_ok=True)\n\n# sphinx_gallery_thumbnail_path = '_static/images/examples/imagenet_corrupted.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Classes\nLater in the tutorial, we will use the following helper classes and functions to track and visualize the progress of our model evaluation on both the clean and corrupted versions of ImageNet, allowing us to observe the differences in performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def display_summary(self):\n        entries = [\" *\"]\n        entries += [str(meter) for meter in self.meters]\n        print(' '.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n\n# Human-readable labels\nsynset_to_name = json.load(urllib.request.urlopen(\n    \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n))\nhuman_readable_labels = synset_to_name\n\ndef visualize_samples(dataset):\n    \"\"\"Vizualize a few samples from the dataset.\"\"\"\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n\n    images, labels = zip(*[dataset[i] for i in range(0, len(dataset), len(dataset) // 15)])\n    images = [img * std + mean for img in images]\n\n    fig, axes = plt.subplots(3, 5, figsize=(12, 7))\n    for ax, img, label in zip(axes.flatten(), images, labels):\n        ax.imshow(img.permute(1, 2, 0).clip(0, 1))\n        ax.set_title(human_readable_labels[label], fontsize=8)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy and Validation Function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\ndef validate(val_loader, model, criterion, device, ttt=False):\n    batch_time = AverageMeter('Time', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    progress = ProgressMeter(len(val_loader), [batch_time, losses, top1, top5], prefix='Test: ')\n\n    model.eval()\n    end = time.time()\n\n    for i, (images, target) in enumerate(val_loader):\n        images = images.to(device)\n        target = target.to(device)\n\n        output = model(images)[0] if ttt else model(images)\n        loss = criterion(output, target)\n\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), images.size(0))\n        top1.update(acc1[0], images.size(0))\n        top5.update(acc5[0], images.size(0))\n\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % 100 == 0:\n            progress.display(i + 1)\n\n    progress.display_summary()\n    return float(top1.avg.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean ImageNet Validation Set\nFirst, let's take a look at the clean validation set. We'll visualize a few sample images. As you can see, the images are clear, and it's relatively easy to recognize the objects they depict.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_dataset = datasets.ImageFolder(\"./imagenet_clean\", transform=transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=8, pin_memory=True)\n\nvisualize_samples(val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretrained Model Evaluation on Clean Data\nNow, let's load a pretrained ResNet-50 model and evaluate its performance on the clean validation set. We'll use the `torchvision` library to load the model. The resulting accuracy, around 77%, aligns with the official accuracy reported by torchvision.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ncriterion = nn.CrossEntropyLoss().to(device)\nmodel = models.resnet50(pretrained=True).to(device)\nmodel.eval()\n\nacc_clean = validate(val_loader, model, criterion, device)\nprint(f\"Clean Accuracy: {acc_clean:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Corrupted ImageNet Validation Set\nNow, let's take a look at the corrupted validation set. We'll visualize a few sample images as well. As you can see, the images are distorted, making it more difficult to recognize the objects they contain. In this example, we use pixelated images as the corruption type, which, as we'll see, has a significant impact on the model's performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "corrupted_dataset = datasets.ImageFolder(\"./imagenet_corrupted\", transform=transform)\ncorrupted_loader = torch.utils.data.DataLoader(corrupted_dataset, batch_size=32, shuffle=True, num_workers=8, pin_memory=True)\n\nvisualize_samples(corrupted_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretrained Model Evaluation on Corrupted Data\nLet's evaluate the pretrained model on the corrupted validation set. As expected, the accuracy drops significantly\u2014from 77% to around 20%\u2014indicating that the model struggles to recognize objects in distorted images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "acc_corrupted = validate(corrupted_loader, model, criterion, device)\nprint(f\"Corrupted Accuracy: {acc_corrupted:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test-Time Adaptation with Tent\nAs we've seen, image corruptions can significantly degrade model performance. To tackle this issue, we can use test-time adaptation techniques. In this example, we'll demonstrate how to apply **TENT** (Test-time Entropy Minimization), which adapts the model to the input data during inference.\n\nTENT works by minimizing the entropy of the model's predictions, encouraging more confident outputs. During inference, the model produces a probability distribution over the classes. TENT computes the entropy of this distribution, then calculates gradients with respect to the parameters of the normalization layers (e.g., BatchNorm) and updates their affine parameters accordingly. This adaptation is performed for a set number of steps, allowing the model to better align with the current input distribution.\n\nUsing TENT with [torch-ttt](https://github.com/nikitadurasov/torch-ttt) is straightforward: simply create a `TentEngine` instance and provide it with the model and optimization parameters such as the learning rate and number of adaptation steps. Even with just a single adaptation step, we observe a notable improvement in accuracy\u2014about 10%\u2014on corrupted ImageNet data compared to the original pretrained ResNet-50. Further tuning of the optimization parameters can lead to even better results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_ttt.engine.tent_engine import TentEngine\n\nengine = TentEngine(\n    model, \n    optimization_parameters={\n        \"lr\": 2e-3, \n        \"num_steps\": 1\n    }\n)\nengine.eval()\n\nacc_tent = validate(corrupted_loader, engine, criterion, device, ttt=True)\nprint(f\"Corrupted Accuracy with Tent: {acc_tent:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}